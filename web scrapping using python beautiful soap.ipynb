{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8408bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "The scraper should paginate through at least the first 5 pages of the site.\n",
    "2. Extract the title and content of each page into a JSON file with keys 'title' and \n",
    "'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdd8c957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markdownify\n",
      "  Downloading markdownify-0.11.6-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in d:\\baby\\phone\\anaa\\lib\\site-packages (from markdownify) (4.12.2)\n",
      "Requirement already satisfied: six<2,>=1.15 in c:\\users\\krith\\appdata\\roaming\\python\\python311\\site-packages (from markdownify) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\baby\\phone\\anaa\\lib\\site-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.4)\n",
      "Installing collected packages: markdownify\n",
      "Successfully installed markdownify-0.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32fec162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping complete. Data saved to 'scraped_data.json'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import markdownify\n",
    "\n",
    "# Define the URL of the website you want to scrape\n",
    "base_url = \"https://blog.tax2win.in/\"\n",
    "\n",
    "# Initialize an empty list to store scraped data\n",
    "data = []\n",
    "\n",
    "# Loop through the first 5 pages\n",
    "for page_number in range(1, 6):\n",
    "    url = f\"{base_url}/{page_number}\"  # Modify this URL structure based on the website's pagination pattern\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract the title of the page\n",
    "        title = soup.title.string.strip()\n",
    "\n",
    "        # Extract the main content of the page\n",
    "        content = soup.find('div', class_='main-content')  # Adjust the class selector based on the website's structure\n",
    "\n",
    "        # Convert HTML tables to markdown\n",
    "        if content:\n",
    "            tables = content.find_all('table')\n",
    "            for table in tables:\n",
    "                table.replace_with(markdownify.markdownify(str(table)))\n",
    "\n",
    "        # Store the data in a dictionary\n",
    "        page_data = {\n",
    "            'title': title,\n",
    "            'content': str(content)\n",
    "        }\n",
    "\n",
    "        # Append the data to the list\n",
    "        data.append(page_data)\n",
    "\n",
    "# Save the scraped data to a JSON file\n",
    "with open('scraped_data.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'scraped_data.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d82452",
   "metadata": {},
   "outputs": [],
   "source": [
    "Convert all html tables into markdown format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "440639c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\baby\\phone\\anaa\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\baby\\phone\\anaa\\lib\\site-packages (4.12.2)\n",
      "Collecting html2text\n",
      "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\baby\\phone\\anaa\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\baby\\phone\\anaa\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\baby\\phone\\anaa\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\baby\\phone\\anaa\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\baby\\phone\\anaa\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Installing collected packages: html2text\n",
      "Successfully installed html2text-2020.1.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 html2text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41443975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No HTML tables found on the page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "# Define the URL of the web page containing HTML tables\n",
    "url = \"https://blog.tax2win.in/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all HTML tables on the page\n",
    "    tables = soup.find_all('tables')\n",
    "\n",
    "    if tables:\n",
    "        # Initialize an empty string to store the Markdown tables\n",
    "        markdown_tables = []\n",
    "\n",
    "        # Iterate through the found tables\n",
    "        for table in tables:\n",
    "            # Convert each HTML table to Markdown\n",
    "            markdown_table = html2text.html2text(str(table))\n",
    "            markdown_tables.append(markdown_table)\n",
    "\n",
    "        # Join the Markdown tables into a single string\n",
    "        markdown_tables_text = \"\\n\\n\".join(markdown_tables)\n",
    "\n",
    "        # Print or save the Markdown tables as needed\n",
    "        print(markdown_tables_text)\n",
    "    else:\n",
    "        print(\"No HTML tables found on the page.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f360780",
   "metadata": {},
   "outputs": [],
   "source": [
    "4 Focus on robust scraping to get clean data - avoid scraping sidebars, ads, social \n",
    "media links, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b99d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main content not found on the page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the web page you want to scrape\n",
    "url = \"https://blog.tax2win.in/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Define a list of HTML element classes or IDs to exclude\n",
    "    exclude_classes = [\"sidebar\", \"ad\", \"social-media\"]\n",
    "    exclude_ids = [\"unwanted-id\"]\n",
    "\n",
    "    # Find and extract the main content of the page\n",
    "    main_content = soup.find('div', class_='main-content')  # Adjust this selector as needed\n",
    "\n",
    "    # Remove unwanted elements based on class or ID\n",
    "    if main_content:\n",
    "        for class_name in exclude_classes:\n",
    "            elements_to_remove = main_content.find_all(class_=class_name)\n",
    "            for element in elements_to_remove:\n",
    "                element.extract()\n",
    "\n",
    "        for id_name in exclude_ids:\n",
    "            element_to_remove = main_content.find(id=id_name)\n",
    "            if element_to_remove:\n",
    "                element_to_remove.extract()\n",
    "\n",
    "        # Now you have the cleaned main content\n",
    "        # You can further process or extract data from it as needed\n",
    "        print(main_content.text)\n",
    "    else:\n",
    "        print(\"Main content not found on the page.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54cfb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic selector for a title\n",
    "title_element = soup.select_one('h1') or soup.select_one('h2') or soup.select_one('.page-title')\n",
    "\n",
    "if title_element:\n",
    "    title = title_element.text\n",
    "else:\n",
    "    title = \"No title found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8adb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
